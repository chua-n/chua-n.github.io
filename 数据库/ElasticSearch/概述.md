## 1. 什么是Elasticsearch

Elasticsearch是一款非常强大的开源搜索引擎，可以帮助我们从海量数据中快速找到需要的内容：

<img src="https://chua-n.gitee.io/blog-images/notebooks/数据库/Elasticsearch/image-20211227234246012.png" alt="image-20211227234246012" style="zoom:45%;" />

Elasticsearch结合kibana, Logstash, Beats，也就是elastic stack(ELK)，被广泛用于在日志数据分析、实时监控等领域：

<img src="https://chua-n.gitee.io/blog-images/notebooks/数据库/Elasticsearch/image-20211227234429928.png" alt="image-20211227234429928" style="zoom:50%;" />

elasticsearch是elsatic stack的核心，负责存储、搜索、分析数据：

<img src="https://chua-n.gitee.io/blog-images/notebooks/数据库/Elasticsearch/image-20211228000741330.png" alt="image-20211228000741330" style="zoom:50%;" />

搜索引擎技术排名；

1. Elasticsearch：开源的分布式搜索引擎
2. Splunk：商业项目
3. Solr：Apache的开源搜索引擎

## 2. Elasticsearch的发展史

[Lucene](https://lucene.apache.org/)是一个java语言的搜索引擎类库，是Apache公司的顶级项目，由DungCutting于1999年研发 。

| Lucene的优点           | Lucene的缺点       |
| ---------------------- | ------------------ |
| 易扩展                 | 只限于Java语言开发 |
| 高性能（基于倒排索引） | 学习曲线陡峭       |
|                        | 不支持水平扩展     |

2004年，Shay Banon基于Lucene开发了Compass；

2010年，Shay Banon重写了Compass，取名为[Elasticsearch](https://www.elastic.co/cn/)。

相比于lucene，elasticsearch具备下列优势：

- 支持分布式，可水平扩展
- 提供Restful接口，可被任何语言调用

## 3. 正向索引和倒排索引

传统数据库（如MySQL）采用**正向索引**——基于记录id创建索引，查询词条时必须先找到记录，而后判断是否包含该词条。如，给下表(tb_goods)中的id创建索引：

<img src="https://chua-n.gitee.io/blog-images/notebooks/数据库/Elasticsearch/IMG_1085.JPG" alt="IMG_1085" style="zoom:36%;" />

而elasticsearch采用**倒排索引**——对文档内容分词，对词条进行索引，并记录词条所在文档的信息，查询时先根据词条查询到文档id，而后获取到文档。即：

- 文档（document）：每条数据就是一个文档
- 词条（term）：文档按照语义分成的词语

<img src="https://chua-n.gitee.io/blog-images/notebooks/数据库/Elasticsearch/IMG_1086.png" alt="IMG_1086" style="zoom:44%;" />

## 4. 文档、字段、索引、映射

elasticsearch是面向文档存储的，文档可以是数据库中的一条商品数据，一个订单信息等，总之，一条数据就是一个文档。

文档数据会被序列化为json格式化存储在elasticsearch中。

<img src="https://chua-n.gitee.io/blog-images/notebooks/数据库/Elasticsearch/image-20211229220350379.png" alt="image-20211229220350379" style="zoom:50%;" />

索引：

- 索引（index）：相同类型的文档的集合
- 映射（mapping）：索引中文档的字段约束信息，类似表的结构约束

<img src="https://chua-n.gitee.io/blog-images/notebooks/数据库/Elasticsearch/image-20211229220756499.png" alt="image-20211229220756499" style="zoom:50%;" />

| MySQL  | Elasticsearch | 说明                                                         |
| ------ | ------------- | ------------------------------------------------------------ |
| Table  | Index         | 索引（index），就是（同类型）文档的集合，类似数据库的表（table） |
| Row    | Document      | 文档（Docuemnt），就是一条条的数据，类似数据库中的行（Row），文档都是JSON格式 |
| Column | Field         | 字段（Field），就是JSON文档中的字段，类似数据库中的列（Column） |
| Schema | Mapping       | Mapping（映射）是索引中文档的约束，例如字段类型约束。类似数据库的表结构（Schema） |
| SQL    | DSL           | DSL是elasticsearch提供的JSON风格的请求语句，用来操作elasticsearch，实现CRUD |

MySQL：擅长事务类型操作，可以确保数据的安全和一致性

Elasticsearch：擅长海量数据的搜索、分析、计算

<img src="https://chua-n.gitee.io/blog-images/notebooks/数据库/Elasticsearch/image-20211229223235967.png" alt="image-20211229223235967" style="zoom:50%;" />

## 5. 分词器

ES在创建倒排索引时需要对文档分词，在搜索时，需要对用户输入内容分词，但默认的分词规则对中文处理并不友好，例如：

```js
POST /_analyze
{
    "analyzer": "standard",
    "text": "黑马程序员学习java太棒了"
}
```

> 语法说明：
>
> - `POST`：请求方式
> - `/_analyze`：请求路径，这里省略了http://192.168.150.101:9200，由kibana自动补充
> - 请求参数：json风格
>     - `analyzer`：分词器类型，这里是默认的standard分词器
>     - `text`：要分词的内容

### IK分词器

对于中文分词，一般会使用[IK分词器](https://github.com/medcl/elasticsearch-analysis-ik)。

> 对于IK分词器的安装，以后再说吧......

IK分词器包含两种模式：

- `ik_smart`：智能切分，粗粒度
- `ik_max_word`：最细切分，细粒度

### IK分词器——拓展词库

要拓展IK分词器，只要修改一个IK分词器目录中的config目录中的IKAnalyzer.cfg.xml文件：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
    <!-- 用户可以在这里配置自己的扩展字典 -->
    <entry key="ext_dict">ext.dic</entry>
</properties>
```

然后在名为ext.dic的文件中，添加想要拓展的词语即可：

```txt
传智播客
奥力给
```

### IK分词器——停用词库

要禁用某些敏感词条，只需要修改一个IK分词器目录中的config目录中的IKAnalyzer.cfg.xml文件：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
    <!-- 用户可以在这里配置自己的扩展字典 -->
    <entry key="ext_dict">ext.dic</entry>
    <!-- 用户可以在这里配置自己的扩展停止字典 -->
    <entry key="ext_stopwords">stopword.dic</entry>
</properties>
```

然后在名为stopword.dic的文件中，添加想要拓展的词语即可：

```text
习大大
```

## 6. 部署ES

> 暂略......

1. 部署单点ES
2. 部署kibana
3. 安装IK分词器
4. 部署ES集群
